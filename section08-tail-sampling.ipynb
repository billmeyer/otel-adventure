{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3806574f-0460-4be2-88ed-66bb7991ed63",
   "metadata": {},
   "source": [
    "# Sampling Traces using OpenTelemetry\n",
    "\n",
    "In this section, we continue exploring sampling options with OpenTelemetry, namely tail-based sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f4491b-ff8b-4281-bbae-e45ce8353de2",
   "metadata": {},
   "source": [
    "## Tail Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d1803-f972-431a-8b50-0eb3a27fb8a1",
   "metadata": {},
   "source": [
    "Tail sampling is where the decision to sample a trace takes place by considering all or most of the spans within the trace. Tail Sampling gives you the option to **sample your traces based on specific criteria derived from different parts of a trace**, which isnâ€™t an option with Head Sampling.\n",
    "\n",
    "Some examples of how you can use Tail Sampling include:\n",
    "\n",
    "* Always sampling traces that contain an **error**\n",
    "* Sampling traces based on overall **latency**\n",
    "* Sampling traces based on the presence or value of specific attributes on one or more spans in a trace\n",
    "    - for example, sampling more traces originating from a newly deployed service\n",
    "* Applying different sampling rates to traces based on certain criteria, such as when traces only come from low-volume services versus traces with high-volume services.\n",
    "\n",
    "The downside to tail sampling today is:\n",
    "\n",
    "1. Can be difficult to implement. Depending on the kind of sampling techniques available to you, it is not always a \"set and forget\" kind of thing.\n",
    "    - As your systems change, so too will your sampling strategies. For a large and sophisticated distributed system, rules that implement sampling strategies can also be large and sophisticated.\n",
    "2. Can be difficult to operate. The component(s) that implement tail sampling must be stateful systems that can **accept and store a large amount of data**.\n",
    "    - Depending on traffic patterns, this can require a large number of compute nodes that all utilize resources differently.\n",
    "    - Furthermore, a tail sampler might need to \"fall back\" to less computationally intensive sampling techniques if it is unable to keep up with the volume of data it is receiving.\n",
    "\n",
    "Because of these factors, it is **critical to monitor tail-sampling components** to ensure that they have the resources they need to make the correct sampling decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020e7991-e1fb-4a8e-a11b-d74803ce2365",
   "metadata": {},
   "source": [
    "## Modify the OpenTelemetry Collector config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674426dd-9ce2-432f-aefc-6d9166f07ec0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Here we will reconfigure our Collector to use the [Tail Sampling Processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/tailsamplingprocessor). This processor supports *only* works with traces.\n",
    "\n",
    "1. Edit `config.yaml`, add `tail_sampling` under the `processors.probabilistic_sampler` section:\n",
    "\n",
    "    ```yaml\n",
    "    processors:\n",
    "      batch:\n",
    "        timeout: 2s\n",
    "      probabilistic_sampler:\n",
    "        sampling_percentage: 15        \n",
    "      tail_sampling:\n",
    "        decision_wait: 10s\n",
    "        num_traces: 100\n",
    "        expected_new_traces_per_sec: 10\n",
    "        decision_cache:\n",
    "          sampled_cache_size: 100000\n",
    "        policies: [\n",
    "            {\n",
    "                name: policy1-always_sample,\n",
    "                type: always_sample\n",
    "            },\n",
    "            {\n",
    "                name: policy2-latency_gt_3000,\n",
    "                type: latency,\n",
    "                latency: {threshold_ms: 3000}\n",
    "            },\n",
    "            {\n",
    "              name: policy3-status_code_error,\n",
    "              type: status_code,\n",
    "              status_code: {status_codes: [ERROR]}\n",
    "            },        \n",
    "        ]\n",
    "    ```\n",
    "    ```\n",
    "    ```\n",
    "    \n",
    "2. Include the `tail_sampling` in the `service.pipelines.traces.processors` section:\n",
    "\n",
    "    ```yaml\n",
    "        traces:\n",
    "          receivers: [otlp]\n",
    "          processors: [tail_sampling, batch]\n",
    "          exporters: [debug/basic, datadog/connector, datadog]\n",
    "    ```\n",
    "    ```\n",
    "    ```\n",
    "\n",
    "    <div class=\"alert alert-block alert-danger\">Be sure to remove the <b>probabilistic_sampler</b> from the <b>services.pipeline.traces.processors</b> section from the previous lab.</div>\n",
    "\n",
    "    > NOTE: We followed a common practice of *defining* multiple processor configurations (i.e., `probabilistic_sampler` *and* `tail_sampling`), but we're only using one of them (`tail_sampling`) in the `service.pipeline.traces.processors` definition. Only the **receivers**, **processors**, and **exporters** that are included in the `service.pipeline` section get used when the Collector is running.\n",
    "\n",
    "3. Save the config and restart the Collector.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b49abf9-4c41-413f-a1e6-81ffbfe06d5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Import OpenTelemetry Modules for Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af7ff5-76ee-48ca-8aab-a6e59923caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry import baggage, trace\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import (\n",
    "    ConsoleSpanExporter,\n",
    "    BatchSpanProcessor\n",
    ")\n",
    "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.trace import Status, StatusCode\n",
    "import datetime, random, socket, time, uuid\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d44f23b-adcd-4885-9ff8-16de9789c01b",
   "metadata": {},
   "source": [
    "## Send Traces to the Collector\n",
    "\n",
    "The code to produce traces has been modified to test out **Tail Sampling policies** in the following ways:\n",
    "\n",
    "* To test **policy2-latency_gt_3000**, random processing delays have been introduced to simulate latency between service invocations.\n",
    "    - This should result in some traces that exceed the maximum allowable latency defined in the policy causing the entire trace to be sampled and sent to Datadog.\n",
    "* To test **policy3-status_code_error**, the random errors that are produced in the **Payment Service** will be used to evaluate spans that return ERROR codes.\n",
    "    - When an error occurs in a span, the entire trace should be sampled and sent to Datadog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0fcc14-a297-44af-9e3b-02be798cc61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTracer(service_name):\n",
    "    provider = TracerProvider(resource=Resource.create({\n",
    "        \"service.name\": service_name,\n",
    "        \"service.instance.id\": str(uuid.uuid4()),        \n",
    "        \"deployment.environment\": \"otel-adventure\",\n",
    "        \"host.name\": socket.gethostname(),\n",
    "    }))\n",
    "    provider.add_span_processor(BatchSpanProcessor(OTLPSpanExporter(endpoint=\"localhost:4317\", insecure=True)))\n",
    "    return trace.get_tracer(\"python\", tracer_provider=provider)\n",
    "    \n",
    "def frontend():\n",
    "    frontend_tracer = getTracer(\"frontend\")\n",
    "    with frontend_tracer.start_as_current_span(\"frontend\") as frontend_span:\n",
    "        print(\"Processing web transaction...\")\n",
    "        start = time.time()\n",
    "        time.sleep(random.random())\n",
    "        handle_checkout()\n",
    "        time.sleep(random.random())\n",
    "    \n",
    "        frontend_span.set_status(Status(StatusCode.OK))\n",
    "        elapsed = int((time.time() - start) * 1000)\n",
    "        print(f\"Transaction complete. {elapsed} msec.\")\n",
    "\n",
    "def handle_checkout():\n",
    "    checkout_tracer = getTracer(\"checkout\")\n",
    "    with checkout_tracer.start_as_current_span(\"checkout\") as checkout_span:\n",
    "        # print(\"Handling checkout...\")\n",
    "        checkout_span.set_attribute(\"order_num\", int(datetime.datetime.timestamp(datetime.datetime.now())*1000) % 100000)\n",
    "        time.sleep(random.random())\n",
    "        handle_payment()\n",
    "        time.sleep(random.random())\n",
    "        handle_shipping()\n",
    "        time.sleep(random.random())\n",
    "    \n",
    "        checkout_span.set_status(Status(StatusCode.OK))\n",
    "        # print(\"Checkout complete.\")\n",
    "        \n",
    "def handle_payment():\n",
    "    payment_tracer = getTracer(\"payment\")\n",
    "    with payment_tracer.start_as_current_span(\"payment\") as payment_span:\n",
    "        # print(\"Handling payment...\")\n",
    "        payment_span.set_attribute(\"payment_id\", str(uuid.uuid4()))\n",
    "        if (random.random() < 0.2):\n",
    "            payment_span.set_status(Status(StatusCode.ERROR, \"Failed to process credit card payment.\"))\n",
    "            print(f\"Simulated error, payment service, trace id: {trace.format_trace_id(payment_span.context.trace_id)}\")\n",
    "        else:\n",
    "            time.sleep(random.random())\n",
    "            payment_span.set_status(Status(StatusCode.OK))\n",
    "        # print(\"Payment complete.\")\n",
    "    \n",
    "def handle_shipping():\n",
    "    shipping_tracer = getTracer(\"shipping\")\n",
    "    with shipping_tracer.start_as_current_span(\"shipping\") as shipping_span:\n",
    "        # print(\"Handling shipping...\")\n",
    "        shipping_span.set_attribute(\"tracking_num\", str(uuid.uuid4()))\n",
    "        time.sleep(random.random())\n",
    "        # print(\"Shipping complete.\")\n",
    "\n",
    "for n in tqdm(range(10)):\n",
    "    frontend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6de10a-b93e-40a6-9ee9-b07b9d6b6878",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Verify Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63890f4c-e181-4b89-bfa8-d08066bfe20b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>DID IT WORK???</div>\n",
    "\n",
    "### How can we verify results?\n",
    "\n",
    "For this example, consider the following output from the previous section:\n",
    "\n",
    "```\n",
    "Processing web transaction...\n",
    "Transaction complete. 4327.771902084351 msec.\n",
    "Processing web transaction...\n",
    "Transaction complete. 2657.130002975464 msec.\n",
    "Processing web transaction...\n",
    "Transaction complete. 3325.37579536438 msec.\n",
    "Processing web transaction...\n",
    "Simulated error, payment service, trace id: c2a0172b8f9f31ab73dc03323fdee573\n",
    "Transaction complete. 3863.502264022827 msec.\n",
    "Processing web transaction...\n",
    "Transaction complete. 4370.552062988281 msec.\n",
    "Processing web transaction...\n",
    "Transaction complete. 3984.522819519043 msec.\n",
    "Processing web transaction...\n",
    "Transaction complete. 4206.815958023071 msec.\n",
    "Processing web transaction...\n",
    "Transaction complete. 4184.7779750823975 msec.\n",
    "Processing web transaction...\n",
    "Transaction complete. 4402.576923370361 msec.\n",
    "Processing web transaction...\n",
    "Transaction complete. 2019.6621417999268 msec.\n",
    "```\n",
    "\n",
    "#### Take note of the following:\n",
    "\n",
    "1. There were were 10 total executions, resulting in 10 total traces produced,\n",
    "2. 8 out of the 10 had execution times > 3000 milliseconds,\n",
    "3. 1 out of the 10 had an ERROR code.\n",
    "\n",
    "### tail_sampling metrics\n",
    "\n",
    "Open the [documentation](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/tailsamplingprocessor/documentation.md) for the `tailsampling` processor.\n",
    "\n",
    "There are numerous metrics emitted by this processor:\n",
    "\n",
    "|Metric|Description|\n",
    "|---|---|\n",
    "|`otelcol_processor_tail_sampling_count_spans_sampled`|Count of spans that were sampled or not per sampling policy|\n",
    "|`otelcol_processor_tail_sampling_count_traces_sampled`|Count of traces that were sampled or not per sampling policy|\n",
    "|`otelcol_processor_tail_sampling_early_releases_from_cache_decision`|Number of spans that were able to be immediately released due to a decision cache hit.|\n",
    "|`otelcol_processor_tail_sampling_global_count_traces_sampled`|Global count of traces that were sampled or not by at least one policy|\n",
    "|`otelcol_processor_tail_sampling_new_trace_id_received`|Counts the arrival of new traces|\n",
    "|`otelcol_processor_tail_sampling_sampling_decision_latency`|Latency (in microseconds) of a given sampling policy|\n",
    "|`otelcol_processor_tail_sampling_sampling_decision_timer_latency`|Latency (in microseconds) of each run of the sampling decision timer|\n",
    "|`otelcol_processor_tail_sampling_sampling_late_span_age`|Time (in seconds) from the sampling decision was taken and the arrival of a late span|\n",
    "|`otelcol_processor_tail_sampling_sampling_policy_evaluation_error`|Count of sampling policy evaluation errors|\n",
    "|`otelcol_processor_tail_sampling_sampling_trace_dropped_too_early`|Count of traces that needed to be dropped before the configured wait time|\n",
    "|`otelcol_processor_tail_sampling_sampling_trace_removal_age`|Time (in seconds) from arrival of a new trace until its removal from memory|\n",
    "|`otelcol_processor_tail_sampling_sampling_traces_on_memory`|Tracks the number of traces current on memory|\n",
    "\n",
    "\n",
    "### Review the Collector metrics\n",
    "\n",
    "1. Either open the Collector's metrics at [http://localhost:8888/metrics](http://localhost:8888/metrics) or execute this shortcut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce183db3-cbcc-4797-aa81-e527cf38637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s http://localhost:8888/metrics | grep otelcol_processor_tail_sampling_count_traces_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d71ed0a-f555-4d44-841b-c1461c2a41fb",
   "metadata": {},
   "source": [
    "2. Search for the metric name: `otelcol_processor_tail_sampling_count_traces_sampled`. There should be five instances of the same metric:\n",
    "\n",
    "    * one for **policy1** where `sampled=\"true\"`,\n",
    "    * two for **policy2**; one where `sampled=\"true\"` and the other where `sampled=false`,\n",
    "    * two for **policy3**; one where `sampled=\"true\"` and the other where `sampled=false`,\n",
    "\n",
    "    ```\n",
    "    ```\n",
    "    ```\n",
    "    # HELP otelcol_processor_tail_sampling_count_traces_sampled Count of traces that were sampled or not per sampling policy\n",
    "    # TYPE otelcol_processor_tail_sampling_count_traces_sampled counter\n",
    "    otelcol_processor_tail_sampling_count_traces_sampled{policy=\"policy1-always_sample\",sampled=\"true\",service_instance_id=\"040a2a22-7be8-4cb8-8fba-9aac40ba86dc\",service_name=\"otelcol-contrib\",service_version=\"0.112.0\"} 10\n",
    "    otelcol_processor_tail_sampling_count_traces_sampled{policy=\"policy2-latency_gt_3000\",sampled=\"false\",service_instance_id=\"040a2a22-7be8-4cb8-8fba-9aac40ba86dc\",service_name=\"otelcol-contrib\",service_version=\"0.112.0\"} 2\n",
    "    otelcol_processor_tail_sampling_count_traces_sampled{policy=\"policy2-latency_gt_3000\",sampled=\"true\",service_instance_id=\"040a2a22-7be8-4cb8-8fba-9aac40ba86dc\",service_name=\"otelcol-contrib\",service_version=\"0.112.0\"} 8\n",
    "    otelcol_processor_tail_sampling_count_traces_sampled{policy=\"policy3-status_code_error\",sampled=\"false\",service_instance_id=\"040a2a22-7be8-4cb8-8fba-9aac40ba86dc\",service_name=\"otelcol-contrib\",service_version=\"0.112.0\"} 9\n",
    "    otelcol_processor_tail_sampling_count_traces_sampled{policy=\"policy3-status_code_error\",sampled=\"true\",service_instance_id=\"040a2a22-7be8-4cb8-8fba-9aac40ba86dc\",service_name=\"otelcol-contrib\",service_version=\"0.112.0\"} 1\n",
    "    ```\n",
    "    ```\n",
    "    ```\n",
    "\n",
    "3. We can verify our sampling policies are working by looking at the metrics for each policy.\n",
    "\n",
    "    - Specifically, **policy1-always_sample** (which samples **everything**) has 10 traces:\n",
    "\n",
    "        ```\n",
    "        ```\n",
    "        ```\n",
    "        otelcol_processor_tail_sampling_count_traces_sampled{policy=\"policy1-always_sample\",sampled=\"true\",service_instance_id=\"040a2a22-7be8-4cb8-8fba-9aac40ba86dc\",service_name=\"otelcol-contrib\",service_version=\"0.112.0\"} 10\n",
    "        ```\n",
    "        ```\n",
    "        ```\n",
    "\n",
    "    - **policy2-latency_gt_3000** should show 8 sampled traces (that took > 3000msecs to execute) and 2 unsampled traces:\n",
    "\n",
    "\n",
    "        ```\n",
    "        ```\n",
    "        ```\n",
    "        otelcol_processor_tail_sampling_count_traces_sampled{policy=\"policy2-latency_gt_3000\",sampled=\"false\",service_instance_id=\"040a2a22-7be8-4cb8-8fba-9aac40ba86dc\",service_name=\"otelcol-contrib\",service_version=\"0.112.0\"} 2\n",
    "        otelcol_processor_tail_sampling_count_traces_sampled{policy=\"policy2-latency_gt_3000\",sampled=\"true\",service_instance_id=\"040a2a22-7be8-4cb8-8fba-9aac40ba86dc\",service_name=\"otelcol-contrib\",service_version=\"0.112.0\"} 8\n",
    "        ```\n",
    "        ```\n",
    "        ```\n",
    "\n",
    "    - Lastly, the **policy3-status_code_error** had 1 sampled trace (that had an error status) and 9 unsampled traces:\n",
    "\n",
    "        ```\n",
    "        ```\n",
    "        ```\n",
    "        otelcol_processor_tail_sampling_count_traces_sampled{policy=\"policy3-status_code_error\",sampled=\"false\",service_instance_id=\"040a2a22-7be8-4cb8-8fba-9aac40ba86dc\",service_name=\"otelcol-contrib\",service_version=\"0.112.0\"} 9\n",
    "        otelcol_processor_tail_sampling_count_traces_sampled{policy=\"policy3-status_code_error\",sampled=\"true\",service_instance_id=\"040a2a22-7be8-4cb8-8fba-9aac40ba86dc\",service_name=\"otelcol-contrib\",service_version=\"0.112.0\"} 1\n",
    "        ```\n",
    "        ```\n",
    "        ```\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6754c6e-538f-45f0-92de-becf07673da9",
   "metadata": {},
   "source": [
    "## What's wrong with what we've done?\n",
    "\n",
    "By sampling spans being sent through the collector, we've impacted our ability to accurately calculate APM statistics (request counts, error counts, and latency measures). These calculations are performed by the `datadog/connector` Connector. This Connector only gets called for what is exported from the `traces` pipeline. Recall the Collector config:\n",
    "\n",
    "```yaml\n",
    "    traces:\n",
    "      receivers: [otlp]\n",
    "      processors: [tail_sampling, batch]\n",
    "      exporters: [debug/basic, datadog/connector, datadog]\n",
    "```      \n",
    "\n",
    "As such, our APM Metrics are now inaccurate.\n",
    "\n",
    "### Solution\n",
    "\n",
    "To solve for this, we need **two** `traces` pipelines. Create a new `traces` pipeline as follows:\n",
    "\n",
    "```yaml\n",
    "    traces/alltraces:\n",
    "      receivers:\n",
    "        - otlp\n",
    "      processors:\n",
    "        - transform/datadog_metadata\n",
    "        - batch\n",
    "      exporters:\n",
    "        - datadog/connector\n",
    "```\n",
    "\n",
    "This pipeline will be used to receive traces from any and all receivers hence the name `traces/alltraces`.\n",
    "\n",
    "Next, we'll modify the original `traces` pipeline to receive **only** from the `datadog/connector` instance:\n",
    "\n",
    "```yaml\n",
    "        traces:\n",
    "          receivers: [datadog/connector]\n",
    "          processors: [tail_sampling, batch]\n",
    "          exporters: [debug/basic, datadog]\n",
    "```\n",
    "\n",
    "and notice that we've removed the `datadog/connector` reference in the `exporters`.\n",
    "\n",
    "The resulting `service.pipelines` section should look like this now:\n",
    "\n",
    "```yaml\n",
    "  pipelines:\n",
    "    metrics:\n",
    "      receivers: [otlp, datadog/connector]\n",
    "      processors: [batch]\n",
    "      exporters: [debug/basic, datadog]\n",
    "\n",
    "    traces/alltraces:\n",
    "      receivers:\n",
    "        - otlp\n",
    "      processors:\n",
    "        - batch\n",
    "      exporters:\n",
    "        - datadog/connector\n",
    "        \n",
    "    traces:\n",
    "      receivers: [otlp]\n",
    "      processors: [tail_sampling, batch]\n",
    "      exporters: [debug/basic, datadog]\n",
    "\n",
    "    logs:\n",
    "      receivers: [otlp]\n",
    "      processors: [batch]\n",
    "      exporters: [debug/basic, datadog]\n",
    "```\n",
    "\n",
    "Restart the Collector for the changes to take effect.\n",
    "\n",
    "## Open Service Catalog\n",
    "\n",
    "View the results by opening [https://app.datadoghq.com/services?env=otel-adventure](https://app.datadoghq.com/services?env=otel-adventure). the **REQUESTS**, **ERROR RATE**, and **P95 LATENCY** were only being calculated for sampled (i.e, egressed) spans which is **not** an accurate accounting of the spans hitting the Collector. With this new configuration in place, these metrics are calculated for all spans sent to the Collector.\n",
    "\n",
    "![image.png](imgs/service-catalog.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f71ebd-5e35-4a66-852a-04704f3593cb",
   "metadata": {},
   "source": [
    "#### End of Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a34014-1305-438b-81cf-35567e7200a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
